{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: [\"삼성준법위, 노사갈등·ESG '실질적 변화'에 힘 싣는다\", \"주택아파트 마련, 담보대출금리비교 활용해서 '알뜰하게'\", '정은보 금감원장 \\\\\"우리銀 횡령 엄정 조치…내부통제 제도 개선도\\\\\"', '대전충남지역 2월 자금사정 밝다', \"쿠팡 '전 상품 품절' 오류 발생\"]\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "folders = [\"Clickbait_Auto\", \"Clickbait_Direct\", \"NonClickbait_Auto\"]\n",
    "categories = [\"EC\", \"ET\", \"GB\", \"IS\", \"LC\", \"PO\", \"SO\"]\n",
    "categories = [\"EC\"]\n",
    "\n",
    "atricles_per_category = 10\n",
    "\n",
    "def extract_part1(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        news_title = data['labeledDataInfo']['newTitle']\n",
    "        news_content = data['sourceDataInfo']['newsContent']\n",
    "        return news_title, news_content\n",
    "    \n",
    "def extract_part2(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        news_title = data['sourceDataInfo']['newsTitle']\n",
    "        news_content = data['labeledDataInfo']['processSentenceInfo']\n",
    "        content = \"\"\n",
    "        for sentence in news_content:\n",
    "            content += sentence[\"sentenceContent\"] + \" \"\n",
    "        return news_title, content\n",
    "\n",
    "\n",
    "news_titles = []\n",
    "news_contents = []\n",
    "labels = []\n",
    "\n",
    "folder_path = './data/Part1'\n",
    "\n",
    "for folder in folders:\n",
    "    for category in categories:\n",
    "        path = folder_path+\"/\"+folder+\"/\"+category\n",
    "        count = 0\n",
    "        for filename in os.listdir(path):\n",
    "            count += 1\n",
    "            if count >= (atricles_per_category *2 if folder == \"NonClickbait_Auto\" else atricles_per_category):\n",
    "                break\n",
    "            file_path = os.path.join(path, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    title, content = extract_part1(file_path)\n",
    "                    news_titles.append(title)\n",
    "                    news_contents.append(content)\n",
    "                    labels.append([0, 1] if folder == \"NonClickbait_Auto\" else [1, 0])\n",
    "                except (json.JSONDecodeError, KeyError) as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "                    continue\n",
    "\n",
    "# folder_path = './data/Part2'\n",
    "\n",
    "# for folder in folders:\n",
    "#     for category in categories:\n",
    "#         path = folder_path+\"/\"+folder+\"/\"+category\n",
    "#         count = 0\n",
    "#         for filename in os.listdir(path):\n",
    "#             count+=1\n",
    "#             if count >= atricles_per_category:\n",
    "#                 break\n",
    "#             file_path = os.path.join(path, filename)\n",
    "#             if os.path.isfile(file_path):\n",
    "#                 try:\n",
    "#                     title, content = extract_part2(file_path)\n",
    "#                     news_titles.append(title)\n",
    "#                     news_contents.append(content)\n",
    "#                     labels.append([0,1]if folder == \"NonClickbait_Auto\" else [1,0])\n",
    "#                 except (json.JSONDecodeError, KeyError) as e:\n",
    "#                     print(f\"Error processing {filename}: {e}\")\n",
    "#                     continue\n",
    "\n",
    "\n",
    "print(\"title:\", news_titles[:5])\n",
    "print(len(news_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    data.append([news_titles[i], news_contents[i], labels[i]])\n",
    "\n",
    "del news_titles\n",
    "del news_contents\n",
    "del labels\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "train_titles = []\n",
    "train_contents = []\n",
    "labels = []\n",
    "\n",
    "for d in data:\n",
    "    train_titles.append(d[0])\n",
    "    train_contents.append(d[1])\n",
    "    labels.append(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아시아나항공 자본 확충 추진...국제선 시동거는 대형항공사\n",
      "과학기술계 정부출연연구기관의 '비정규직 정규직 전환'이 내년까지 본격적으로 추진된다.\n",
      "전환대상자는 상시·지속 업무를 수행하는 현재 비정규직 근무자로, 필요하다면 '경쟁채용' 방식이 도입된다.\n",
      "과학기술정보통신부는 이러한 내용을 담은 '출연연 비정규직의 정규직 전환 가이드라인'을 24일 발표했다.\n",
      "이는 지난 7월 관계부처 합동으로 발표한 '공공부문 정규직 전환 가이드라인'의 후속조치다.\n",
      "출연연은 기관별 임무에 따라 R&D(연구개발)를 수행하면서 전문 연구인력, 연구보조 인력 등을 비정규직 형태로 운영해왔다.\n",
      "출연연 내 비정규직은 운영 방식과 비정규직 근무자가 수행하는 업무 특성이 다양해 일률적인 기준을 적용하기엔 무리가 있다.\n",
      "과기정통부는 정부 정책 취지인 '상시·지속업무는 정규직으로 전환한다'는 원칙은 지키되 상시·지속업무의 범위를 확대했다.\n",
      "그 예로 기간이 한정된 연구 프로젝트 수행을 위해 채용한 비정규직도 통상적으로 계약을 연장하며 다년 간 또는 다수 프로젝트를 수행하기 때문에 상시·지속 업무로 간주할 수 있다.\n",
      "또 연구 수행 시 안전과 관련이 깊거나 폭발물·유해물질 처리 등 위험이 큰 업무는 정규직화 한다.\n",
      "우선 전환대상 업무가 결정되면 해당 업무를 수행 중인 현 근무자를 대상으로 최소한의 평가절차를 거쳐 정규직으로 전환한다.\n",
      "평가과정에서 연구업무의 전문성 등 합리적인 사유가 있다면, 경쟁채용 방식이 적용될 수도 있다.\n",
      "그러나 '정규직 전환 심의위원회'에서 기관이 제시한 사유와 현 근무자의 의견 등을 심의해 정당성을 확보돼야만 한다.\n",
      "이날 발표된 가이드라인에 따라, 출연연은 12월까지 정규직 전환 계획을 확정해야 한다.\n",
      "계획에 따라 출연연은 기간제의 경우 내년 3월까지 전환을 완료하고, 파견·용역직의 경우 내년 이후 민간업체의 계약 기간 종료 시점에 맞춰 전환해야 한다.\n",
      "기관별 계획이 취합돼야 전체 대상자 수를 알 수 있을 전망이다.\n",
      "반면 박사후연구원, 학생연구원 등 정규 직업을 갖기 전 '연수'가 목적인 근무자들은 이번 정규직 전환 대상자에서 제외됐다.\n",
      "과기정통부는 앞으로 이들을 '연수직(가칭)'으로 분류해 복리후생 개선, 고용안정, 처우개선을 추진할 방침이다.\n",
      "이진규 제1차관은 \\\"우수인력 확보가 무엇보다 중요한 연구기관의 특성과 출연연 연구일자리 진입 경쟁에서 '경쟁기회 공정성'을 고려해 '현 근무자' 전환이 아닌 경쟁채용을 확대해야 한다는 목소리가 있었던 것이 사실이다\\\"라면서도 \\\"연구기관이라도 현재 연구 성과에 기여하고 있는 '현 근무자'의 고용안정을 우선으로 고려하는 것이 정책의 취지에 더 부합한다고 판단했다\\\"고 설명했다.\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_titles[16])\n",
    "print(train_contents[16])\n",
    "print(labels[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "cnt=0 # # of clickbait\n",
    "for label in labels:\n",
    "    if label == [1,0]:\n",
    "        cnt+=1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class NewsConsistencyDataset(Dataset):\n",
    "    def __init__(self, headlines, article_bodies, labels, tokenizer, max_headline_len=128, max_article_len=512):\n",
    "        self.headlines = headlines\n",
    "        self.article_bodies = article_bodies\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_headline_len = max_headline_len\n",
    "        self.max_article_len = max_article_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "    def __getitem__(self, idx):\n",
    "        headline = str(self.headlines[idx])\n",
    "        article = str(self.article_bodies[idx])\n",
    "        \n",
    "        headline_encoding = self.tokenizer.encode_plus(\n",
    "            headline,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_headline_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        article_encoding = self.tokenizer.encode_plus(\n",
    "            article,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_article_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'headline_ids': headline_encoding['input_ids'].squeeze(0),\n",
    "            'headline_mask': headline_encoding['attention_mask'].squeeze(0),\n",
    "            'article_ids': article_encoding['input_ids'].squeeze(0),\n",
    "            'article_mask': article_encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None):\n",
    "        attn_output, _ = self.attention(query, key, value, key_padding_mask=key_padding_mask)\n",
    "        attn_output = self.norm(attn_output + query)\n",
    "        return self.linear(attn_output)\n",
    "\n",
    "class NewsConsistencyChecker(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\n",
    "        self.bert = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.reduced_size = self.hidden_size // 2\n",
    "        \n",
    "        self.headline_projection = nn.Linear(self.hidden_size, self.reduced_size)\n",
    "        self.article_projection = nn.Linear(self.hidden_size, self.reduced_size)\n",
    "        \n",
    "        self.cross_attention_1 = CrossAttention(self.reduced_size)\n",
    "        self.cross_attention_2 = CrossAttention(self.reduced_size)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.reduced_size * 4, self.reduced_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.reduced_size, self.reduced_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.reduced_size // 2, 2),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, headline_ids, headline_mask, article_ids, article_mask):\n",
    "        headline_output = self.bert(\n",
    "            headline_ids,\n",
    "            attention_mask=headline_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        article_output = self.bert(\n",
    "            article_ids,\n",
    "            attention_mask=article_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        headline_embeddings = headline_output.last_hidden_state\n",
    "        article_embeddings = article_output.last_hidden_state\n",
    "        \n",
    "        headline_proj = self.headline_projection(headline_embeddings)\n",
    "        article_proj = self.article_projection(article_embeddings)\n",
    "        \n",
    "        headline_attended = self.cross_attention_1(\n",
    "            headline_proj, article_proj, article_proj, key_padding_mask=~article_mask.bool(),\n",
    "        )\n",
    "        article_attended = self.cross_attention_1(\n",
    "            article_proj, headline_proj, headline_proj, key_padding_mask=~headline_mask.bool(),\n",
    "        )\n",
    "        \n",
    "        headline_attended = self.cross_attention_2(\n",
    "            headline_attended, article_attended, article_attended, key_padding_mask=~article_mask.bool(),\n",
    "        )\n",
    "        article_attended = self.cross_attention_2(\n",
    "            article_attended, headline_attended, headline_attended, key_padding_mask=~headline_mask.bool(),\n",
    "        )\n",
    "        \n",
    "        headline_pool = torch.mean(headline_attended, dim=1)\n",
    "        article_pool = torch.mean(article_attended, dim=1)\n",
    "        headline_max, _ = torch.max(headline_attended, dim=1)\n",
    "        article_max, _ = torch.max(article_attended, dim=1)\n",
    "        \n",
    "        combined = torch.cat([headline_pool, article_pool, headline_max, article_max], dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 0: The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [29, 1024].  Tensor sizes: [1, 512]\n",
      "Headline shape: torch.Size([29, 128])\n",
      "Article shape: torch.Size([29, 1024])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [8, 1024].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m NewsConsistencyChecker()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 81\u001b[0m train_model(model, train_loader, val_loader, device)\n",
      "Cell \u001b[0;32mIn[11], line 51\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, num_epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     48\u001b[0m }\n\u001b[1;32m     49\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 51\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     54\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/clickbait/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/clickbait/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 101\u001b[0m, in \u001b[0;36mNewsConsistencyChecker.forward\u001b[0;34m(self, headline_ids, headline_mask, article_ids, article_mask)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, headline_ids, headline_mask, article_ids, article_mask):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# BERT embedding\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     headline_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m     96\u001b[0m         headline_ids,\n\u001b[1;32m     97\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mheadline_mask,\n\u001b[1;32m     98\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[0;32m--> 101\u001b[0m     article_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m    102\u001b[0m         article_ids,\n\u001b[1;32m    103\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39marticle_mask,\n\u001b[1;32m    104\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    107\u001b[0m     headline_embeddings \u001b[38;5;241m=\u001b[39m headline_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    108\u001b[0m     article_embeddings \u001b[38;5;241m=\u001b[39m article_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/miniconda3/envs/clickbait/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/clickbait/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/clickbait/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1073\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1072\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m-> 1073\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m buffered_token_type_ids\u001b[38;5;241m.\u001b[39mexpand(batch_size, seq_length)\n\u001b[1;32m   1074\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [8, 1024].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs = {\n",
    "                'headline_ids': batch['headline_ids'].to(device),\n",
    "                'headline_mask': batch['headline_mask'].to(device),\n",
    "                'article_ids': batch['article_ids'].to(device),\n",
    "                'article_mask': batch['article_mask'].to(device)\n",
    "            }\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            try:\n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f'Epoch {epoch + 1}, Batch {batch_idx + 1}, Loss: {loss.item():.4f}')\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                print(f\"Headline shape: {inputs['headline_ids'].shape}\")\n",
    "                print(f\"Article shape: {inputs['article_ids'].shape}\")\n",
    "                continue\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {\n",
    "                    'headline_ids': batch['headline_ids'].to(device),\n",
    "                    'headline_mask': batch['headline_mask'].to(device),\n",
    "                    'article_ids': batch['article_ids'].to(device),\n",
    "                    'article_mask': batch['article_mask'].to(device)\n",
    "                }\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}:')\n",
    "        print(f'Training Loss: {total_loss / len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\n",
    "\n",
    "dataset = NewsConsistencyDataset(\n",
    "    headlines=train_titles,\n",
    "    article_bodies=train_contents,\n",
    "    labels=labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_headline_len=128,\n",
    "    max_article_len=1024,\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=35, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=35)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NewsConsistencyChecker().to(device)\n",
    "\n",
    "train_model(model, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = NewsConsistencyDataset(\n",
    "    headlines=[\"\"],\n",
    "    article_bodies=[\"\"],\n",
    "    labels=[1],\n",
    "    tokenizer=tokenizer,\n",
    "    max_headline_len=128,\n",
    "    max_article_len=1024,\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(real_data, batch_size=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        inputs = {\n",
    "            'headline_ids': batch['headline_ids'].to(device),\n",
    "            'headline_mask': batch['headline_mask'].to(device),\n",
    "            'article_ids': batch['article_ids'].to(device),\n",
    "            'article_mask': batch['article_mask'].to(device)\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        print(f\"낚시성 기사일 확률: {round(outputs.numpy()[0][0]*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_state_dict.pt')\n",
    "# loaded_model = NewsConsistencyChecker()\n",
    "# loaded_model.load_state_dict(torch.load('model_state_dict.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clickbait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
